{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-11T02:19:06.597661Z","iopub.execute_input":"2023-09-11T02:19:06.598517Z","iopub.status.idle":"2023-09-11T02:19:07.035525Z","shell.execute_reply.started":"2023-09-11T02:19:06.598481Z","shell.execute_reply":"2023-09-11T02:19:07.034503Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**Unzip files**","metadata":{}},{"cell_type":"code","source":"!unzip datasource.zip -d dealership_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Set path**","metadata":{}},{"cell_type":"code","source":"tmpfile    = \"dealership_temp.tmp\"               # file used to store all extracted data\nlogfile    = \"dealership_logfile.txt\"            # all event logs will be stored in this file\ntargetfile = \"dealership_transformed_data.csv\"   # file where transformed data is stored","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Question 1: CSV Extract Function**","metadata":{}},{"cell_type":"code","source":"def extract_from_csv (file_to_process):\n    dataframe = pd.read_csv(file_to_process)\n    return datarame","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Question 2: JSON Extract Function**","metadata":{}},{"cell_type":"code","source":"def extract_from_json (file_to_process):\n    dataframe = pd.read_json(file_to_process,lines=True)\n    return dataframe","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Question 3: XML Extract Function**","metadata":{}},{"cell_type":"code","source":"def extract_from_xml (file_to_process):\n    dataframe = pd.DataFrame(columns=['car_model','year_manufacture','price','fuel'])\n    tree = ET.parse(file_to_process)\n    root = tree.getroot()\n    for person in root:\n        car_model = person.find(\"car_model\").text\n        year_of_manufacture = int(person.find(\"year_of_manufacture\").text)\n        price = float(person.find(\"price\").text)\n        fuel = person.find(\"fuel\").text\n        dataframe - dataframe.append({\"car_model\":car_model,\"year_of_manufacture\":year_of_manufacture,\"price\":price,\"fuel\":fuel}, ignore_index=True)\n    return dataframe","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Question 4: Extract Function**","metadata":{}},{"cell_type":"code","source":"def extract():\n    extracted_data = pd.DataFrame(columns=['car_model','year_of_manufacture','price', 'fuel']) # create an empty data frame to hold extracted data\n    \n    #process all csv files\n    for csvfile in glob.glob(\"dealership_data/*.csv\"):\n        extracted_data = extracted_data.append(extract_from_csv, ignore_index=True)\n        \n    #process all json files\n    for jsonfile in glob.glob(\"dealership_data/*.json\"):\n        extracted_data = extracted_data.append(extract_from_json, ignore_index=True)\n    \n    #process all xml files\n    for xmlfile in glob.glob(\"dealership_data/*.xml\"):\n        extracted_data = extracted_data.append(extract_from_xml, ignore_index=True)\n        \n    return extracted_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Question 5: Transform**","metadata":{}},{"cell_type":"code","source":"# Add the transform function below\ndef transform(data):\n    data['price'] = round(data.price,2)\n    return data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Question 6: Load**","metadata":{}},{"cell_type":"code","source":"# Add the load function below\ndef load(targetfile,data_to_load):\n    data_to_load.to_csv(targetfile)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Question 7: Log**\n\nMake sure to change the name of the logfile to the one specified in the set paths section. Change the timestamp order to Hour-Minute-Second-Monthname-Day-Year.","metadata":{}},{"cell_type":"code","source":"# Add the log function below\ndef log(message):\n    timestamp_format = '%H:%M:%S-%h-%d-%Y' # Hour-Minute_Second-Monthname-Day-Year\n    now = datetime.now() # get current timestamp\n    timestamp = now.strftime(timestamp_format)\n    with open(\"dealership_logfile.txt\",\"a\") as f:\n        f.write(timestamp + ',' + message + '\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Question 8: ETL Process**\n\nRun all functions to extract, transform, and load the data. Make sure to log all events using the `log` function. Place your code under each comment.","metadata":{}},{"cell_type":"code","source":"# Log that you have started the ETL process\nlog (\"ETL Job Started\")\n\n# Log that you have started the Extract step\nlog(\"Extract phase Started\")\n\n# Call the Extract function\nextracted_data = extract()\n\n# Log that you have completed the Extract step\nlog(\"Extract phase Ended\")\nextracted_data\n\n# Log that you have started the Transform step\nlog(\"Transform phase Started\")\n\n# Call the Transform function\ntransformed_data = transform(extracted_data)\n\n# Log that you have completed the Transform step\nlog(\"Transform phase Ended\")\ntransformed_data\n\n# Log that you have started the Load step\nlog(\"Load phase Started\")\n# Call the Load function\nload(targetfile, transformed_data)\n# Log that you have completed the Load step\nlog(\"Load phase Ended\")\n\n# Log that you have completed the ETL process\nlog(\"ETL Job Ended\")","metadata":{},"execution_count":null,"outputs":[]}]}